{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1460f-8d38-400a-aaf8-09fe8d3c57f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This si also creating us and endpoint via gradio and its hosted on a Vertax AI workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16695bcf-e88b-4277-986a-a0e40e89ad28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'> odict_keys(['out', 'aux'])\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://a50bdbbf43f8a38ea5.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a50bdbbf43f8a38ea5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/blocks.py\", line 2018, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/blocks.py\", line 1567, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/utils.py\", line 846, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/tmp/ipykernel_41613/2439344890.py\", line 52, in segment_image\n",
      "    gray_image = ImageOps.grayscale(Image.fromarray(image))\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/PIL/Image.py\", line 3266, in fromarray\n",
      "    arr = obj.__array_interface__\n",
      "AttributeError: 'NoneType' object has no attribute '__array_interface__'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import numpy as np\n",
    "import requests\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "\n",
    "\n",
    "image_url = \"https://farm1.staticflickr.com/6/9606553_ccc7518589_z.jpg\"\n",
    "image = np.array(Image.open(requests.get(image_url, stream=True).raw))\n",
    "rgb_img = np.float32(image) / 255\n",
    "input_tensor = preprocess_image(rgb_img,\n",
    "                                mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "# Taken from the torchvision tutorial\n",
    "# https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html\n",
    "model = deeplabv3_resnet50(pretrained=True, progress=False)\n",
    "model = model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    input_tensor = input_tensor.cuda()\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(type(output), output.keys())\n",
    "class SegmentationModelOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, model): \n",
    "        super(SegmentationModelOutputWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)[\"out\"]\n",
    "    \n",
    "model = SegmentationModelOutputWrapper(model)\n",
    "\n",
    "\n",
    "# A simple mock function to mimic segmentation (thresholding for example)\n",
    "def segment_image(image):\n",
    "    # Convert to grayscale (PIL image)\n",
    "    gray_image = ImageOps.grayscale(Image.fromarray(image))\n",
    "    \n",
    "    # Convert to NumPy array\n",
    "    img_array = np.array(gray_image)\n",
    "    rgb_img = np.float32(image) / 255\n",
    "    input_tensor = preprocess_image(rgb_img,\n",
    "                                    mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "    output = model(input_tensor)\n",
    "    normalized_masks = torch.nn.functional.softmax(output, dim=1).cpu()\n",
    "    car_mask = normalized_masks[0, :, :, :].argmax(axis=0).detach().cpu().numpy()\n",
    "    car_mask_uint8 = 255 * np.uint8(car_mask == 0)\n",
    "    segmented_image = np.float32(car_mask == 0)\n",
    "    \n",
    "    # Apply a simple threshold to mimic segmentation\n",
    "    #threshold_value = 128\n",
    "    #segmentation_mask = img_array > threshold_value\n",
    "\n",
    "    # Convert segmentation mask back to a PIL image\n",
    "    #segmented_image = Image.fromarray(np.uint8(car_mask_float * 255))\n",
    "\n",
    "    return segmented_image\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(fn=segment_image, \n",
    "                     inputs=\"image\", \n",
    "                     outputs=\"image\", \n",
    "                     title=\"Image Segmentation\",\n",
    "                     description=\"Upload an image and see the segmentation result.\")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f176380-c2d2-4b30-9285-ddf35a5da205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
